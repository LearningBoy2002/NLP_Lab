{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 1**"
      ],
      "metadata": {
        "id": "ABgUGi9du18W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGz22ZLI_pef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f43fd3-9c0e-4eec-929c-91344691dbd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good morning everyone.', 'Today we will study NLTK.']\n",
            "['Good', 'morning', 'everyone', '.', 'Today', 'we', 'will', 'study', 'NLTK', '.']\n",
            "Good morning everyone.\n",
            "Good\n",
            "morning\n",
            "everyone\n",
            ".\n",
            "Today we will study NLTK.\n",
            "Today\n",
            "we\n",
            "will\n",
            "study\n",
            "NLTK\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "#1.To perfrome Tokenization of a given sentence\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "example=\"Good morning everyone. Today we will study NLTK.\"\n",
        "print(sent_tokenize(example))\n",
        "print(word_tokenize(example))\n",
        "for i in (sent_tokenize(example)):\n",
        "  print (i)\n",
        "  for j in (word_tokenize(i)):\n",
        "    print (j)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 2**"
      ],
      "metadata": {
        "id": "EORyJ1iVvKDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. To find the stopwords of a given text\n",
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words)\n",
        "words=word_tokenize(example)\n",
        "remove_stopwords=[]\n",
        "for i in words:\n",
        "  if i not in stop_words:\n",
        "    remove_stopwords.append(i)\n",
        "print(remove_stopwords)\n",
        "remove_words=[]\n",
        "remove_words= [i for i in words if i in stop_words]\n",
        "print(remove_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W51ZTO3vvREa",
        "outputId": "c2f50660-c608-460b-d7dc-abab8430c090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'has', 'again', 'the', \"should've\", 'his', 'yourself', 'had', 'but', 'you', 'up', 'because', 'each', 'of', 'that', 'been', 'who', 'those', 'against', 'not', 'i', 'he', 'at', 'in', 'are', 'yours', 'from', 'into', 'below', 'be', 'during', 'it', 'having', 'ours', 'an', 'between', 'were', 'this', 'myself', 'your', \"doesn't\", 'off', \"won't\", 'most', 'whom', 'for', \"that'll\", 'ma', 'nor', 'and', \"shan't\", 'just', 'm', 'don', 'should', 'didn', \"shouldn't\", \"haven't\", 'which', 'once', 'while', 'some', 'through', 't', 'on', 'me', 'him', 'only', 'her', 'before', 'where', 'after', 'himself', \"didn't\", 'all', 'will', 'have', 'same', 'hasn', 'or', \"needn't\", 'then', \"you'd\", 'was', 'with', \"don't\", 'mightn', 'couldn', 'few', 'my', 'theirs', \"hasn't\", 'their', \"you've\", 'out', 'a', 'd', 'll', 'they', 'by', 'being', 'now', 'both', \"mightn't\", \"wouldn't\", 're', 'themselves', \"hadn't\", 'she', 'no', 'ain', \"you're\", 'under', 'until', 'how', 'than', 've', \"weren't\", 'itself', 'weren', 'why', 'haven', \"aren't\", 'doing', 'o', 'about', 'when', 'wouldn', 'do', 'needn', 'them', 'can', 'so', 'to', 'did', 'there', \"isn't\", 'our', 'wasn', 'very', 'other', 'over', 'is', 'hadn', 'mustn', 'if', 'hers', 'above', 'herself', 'own', 'won', 'ourselves', 'what', 'any', 'does', 'am', 'further', 'y', 'down', \"she's\", 'aren', \"mustn't\", 'too', 'these', 'shan', \"you'll\", \"wasn't\", \"it's\", 'more', 'shouldn', 'here', 'such', 'doesn', 's', 'we', 'isn', 'its', \"couldn't\", 'as', 'yourselves'}\n",
            "['Good', 'morning', 'everyone', '.', 'Today', 'study', 'NLTK', '.']\n",
            "['we', 'will']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 3**"
      ],
      "metadata": {
        "id": "HSwOfW4Zw0zV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3. To perform stemming operation using NLTK\n",
        "import nltk.stem\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "ps=PorterStemmer()\n",
        "sb=SnowballStemmer((\"english\"))\n",
        "words= [\"Computer\",\"Computerization\",\"Computerize\"]\n",
        "for i in words:\n",
        "  print(i+\":\" +ps.stem(i))\n",
        "print(\"---------\")\n",
        "for i in words:\n",
        "  print(i+\":\" +sb.stem(i))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfMmEw58wOEh",
        "outputId": "ba232cc3-b468-4bbd-ef6c-ebff605219d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computer:comput\n",
            "Computerization:computer\n",
            "Computerize:computer\n",
            "---------\n",
            "Computer:comput\n",
            "Computerization:computer\n",
            "Computerize:computer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 4**"
      ],
      "metadata": {
        "id": "4gmgK90XxOks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. To perfrom POS tagging of a given text\n",
        "import nltk.stem\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download('state_union')\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "text= state_union.raw(\"2005-GWBush.txt\")\n",
        "text=\"Good morning everyone. Today we will study NLTK.\"\n",
        "custom_sent_tokenizer1= sent_tokenize(text)\n",
        "print(custom_sent_tokenizer1)\n",
        "custom_sent_tokenizer= PunktSentenceTokenizer(text)\n",
        "tokenized= custom_sent_tokenizer.tokenize(text)\n",
        "print(tokenized)\n",
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized:\n",
        "      words= nltk.word_tokenize(i)\n",
        "      tagged= nltk.pos_tag(words)\n",
        "      print(tagged)\n",
        "  except Exception as e:\n",
        "   print(str(e))\n",
        "process_content()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mw8d45nxBqD",
        "outputId": "7026fabd-10d2-4144-dd43-cd58eb29341d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good morning everyone.', 'Today we will study NLTK.']\n",
            "['Good morning everyone.', 'Today we will study NLTK.']\n",
            "[('Good', 'JJ'), ('morning', 'NN'), ('everyone', 'NN'), ('.', '.')]\n",
            "[('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('study', 'VB'), ('NLTK', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 5**"
      ],
      "metadata": {
        "id": "G4mSIqFxxl4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Take two sentence from 'austen-emma.text' and then perfrom stemming\n",
        "import nltk\n",
        "import requests\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Corrected URL\n",
        "url = \"https://raw.githubusercontent.com/fbkarsdorp/Python-Course/master/data/austen-emma.txt\"\n",
        "\n",
        "response = requests.get(url)\n",
        "text = response.text  # Use .text attribute to get the content\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Tokenizing sentences\n",
        "tokenized_sentences = sent_tokenize(text)\n",
        "\n",
        "# Selecting specific sentences\n",
        "selected_sentences = tokenized_sentences[3:5]\n",
        "\n",
        "for sentence in selected_sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    stemmed_words = [ps.stem(word) for word in words]\n",
        "    stemmed_sentence = ' '.join(stemmed_words)\n",
        "    print(stemmed_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9Yl4u0CxgtX",
        "outputId": "12e966e1-6f08-485a-bc44-d29929dda519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sixteen year had miss taylor been in mr. woodhous 's famili , less as a gover than a friend , veri fond of both daughter , but particularli of emma .\n",
            "between _them_ it wa more the intimaci of sister .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Take two sentence then perfrom stemming\n",
        "import nltk.stem\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "ps=PorterStemmer()\n",
        "sb=SnowballStemmer((\"english\"))\n",
        "words= [\"Computer\",\"Computerization\",\"Computerize\"]\n",
        "for i in words:\n",
        "  print(i+\":\" +ps.stem(i))\n",
        "print(\"---------\")\n",
        "for i in words:\n",
        "  print(i+\":\" +sb.stem(i))"
      ],
      "metadata": {
        "id": "7v2TQZ83CCAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6e224b-c632-443c-f3fb-c782fe76d3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computer:comput\n",
            "Computerization:computer\n",
            "Computerize:computer\n",
            "---------\n",
            "Computer:comput\n",
            "Computerization:computer\n",
            "Computerize:computer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6,11 Ngram in given text or sentence\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#import bigrams, trigrams, ngrams\n",
        "text=\"Good morning everyone. Today we will study NLTK. Good afternoon everyone.\"\n",
        "tokens=nltk.word_tokenize(text)\n",
        "# bigrams_list=list(nltk.bigrams(tokens))\n",
        "# print(bigrams_list)\n",
        "\n",
        "ngrams_list=list(nltk.ngrams(tokens,3))##\n",
        "print(ngrams_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xei8FKQyDxrd",
        "outputId": "e61323b1-6ba6-4683-d5c0-c5cb175bc589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Good', 'morning', 'everyone'), ('morning', 'everyone', '.'), ('everyone', '.', 'Today'), ('.', 'Today', 'we'), ('Today', 'we', 'will'), ('we', 'will', 'study'), ('will', 'study', 'NLTK'), ('study', 'NLTK', '.'), ('NLTK', '.', 'Good'), ('.', 'Good', 'afternoon'), ('Good', 'afternoon', 'everyone'), ('afternoon', 'everyone', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7.  To perfrom  Tokenization and POS tagging from 'austen-emma.text'\n",
        "import nltk.stem\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download('state_union')\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "text= state_union.raw(\"2005-GWBush.txt\")\n",
        "text=\"Good morning everyone. Today we will study NLTK.\"\n",
        "custom_sent_tokenizer1= sent_tokenize(text)\n",
        "print(custom_sent_tokenizer1)\n",
        "custom_sent_tokenizer= PunktSentenceTokenizer(text)\n",
        "tokenized= custom_sent_tokenizer.tokenize(text)\n",
        "print(tokenized)\n",
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized:\n",
        "      words= nltk.word_tokenize(i)\n",
        "      tagged= nltk.pos_tag(words)\n",
        "      print(tagged)\n",
        "  except Exception as e:\n",
        "   print(str(e))\n",
        "process_content()\n"
      ],
      "metadata": {
        "id": "Wx7ANAtEET1V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8561a8-e939-4d81-aeff-4601e0951515"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good morning everyone.', 'Today we will study NLTK.']\n",
            "['Good morning everyone.', 'Today we will study NLTK.']\n",
            "[('Good', 'JJ'), ('morning', 'NN'), ('everyone', 'NN'), ('.', '.')]\n",
            "[('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('study', 'VB'), ('NLTK', 'NNP'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. To perfrom the named Entity recognition using the chunking method\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk import ne_chunk\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "train_text=\" Sundaram Pichai is a CEO of GOOGLE Company\"\n",
        "sample_text=\" Sundaram Pichai is a CEO of GOOGLE Company\"\n",
        "custom_sent_tokenizer= PunktSentenceTokenizer(train_text)\n",
        "tokenized= custom_sent_tokenizer.tokenize(sample_text)\n",
        "print(tokenized)\n",
        "def process_content():\n",
        "  try:\n",
        "     for i in tokenized:\n",
        "        words= nltk.word_tokenize(i)\n",
        "        tagged= nltk.pos_tag(words)#('averaged perceptron tagger')\n",
        "        chunks=nltk.ne_chunk(tagged)\n",
        "        print(chunks)\n",
        "        chunks.pretty_print(unicodelines=True)\n",
        "  except Exception as e:\n",
        "        print(str(e))\n",
        "process_content()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzOBwQ5UIkKF",
        "outputId": "dd8c1c31-919c-450e-d46f-d4bc5152ed23"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' Sundaram Pichai is a CEO of GOOGLE Company']\n",
            "(S\n",
            "  (PERSON Sundaram/NNP)\n",
            "  (PERSON Pichai/NNP)\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  CEO/NN\n",
            "  of/IN\n",
            "  (ORGANIZATION GOOGLE/NNP Company/NNP))\n",
            "                     S                                                              \n",
            "  ┌─────┬─────┬──────┼────────┬───────────┬──────────────────────┐                   \n",
            "  │     │     │      │      PERSON      PERSON              ORGANIZATION            \n",
            "  │     │     │      │        │           │          ┌───────────┴────────────┐      \n",
            "is/VBZ a/DT CEO/NN of/IN Sundaram/NNP Pichai/NNP GOOGLE/NNP              Company/NNP\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9 To perform lemmatization of some given text\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wrd=WordNetLemmatizer()\n",
        "print(wrd.lemmatize(\"better\",pos=\"a\"))\n",
        "print(wrd.lemmatize(\"better\",pos=\"n\"))\n",
        "print(wrd.lemmatize(\"is\",pos=\"v\"))\n",
        "print(wrd.lemmatize(\"is\",pos=\"r\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra8pbqZyCApW",
        "outputId": "9f489a23-565a-4f0a-8a5f-85eab49cb7d0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n",
            "better\n",
            "be\n",
            "is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10\n",
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "from nltk import FreqDist, NaiveBayesClassifier\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.classify import accuracy\n",
        "import random\n",
        "\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "random.shuffle(documents)\n",
        "\n",
        "all_words = FreqDist(w.lower() for w in movie_reviews.words())\n",
        "word_features = list(all_words)[:1000]\n",
        "\n",
        "def document_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['contains({})'.format(word)] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "train_set, test_set = featuresets[:900], featuresets[900:]  # Corrected the splitting\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Print accuracy and show most informative features\n",
        "print('Accuracy:', accuracy(classifier, test_set) * 100)\n",
        "classifier.show_most_informative_features(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq0l7mfiHOjK",
        "outputId": "558098cc-5e28-4023-f31d-c3a635019adc"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 76.0\n",
            "Most Informative Features\n",
            "        contains(stupid) = True              neg : pos    =      4.4 : 1.0\n",
            "     contains(memorable) = True              pos : neg    =      4.0 : 1.0\n",
            "         contains(worst) = True              neg : pos    =      3.8 : 1.0\n",
            "           contains(key) = True              pos : neg    =      3.6 : 1.0\n",
            "          contains(mess) = True              neg : pos    =      3.5 : 1.0\n",
            "         contains(solid) = True              pos : neg    =      3.2 : 1.0\n",
            "     contains(perfectly) = True              pos : neg    =      3.2 : 1.0\n",
            "     contains(excellent) = True              pos : neg    =      3.2 : 1.0\n",
            "       contains(overall) = True              pos : neg    =      3.0 : 1.0\n",
            "      contains(supposed) = True              neg : pos    =      2.9 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. to perfrom Chuunking operation\n",
        "from sklearn import tree\n",
        "from nltk.tree import *\n",
        "text = \"A dog chased a cat\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)\n",
        "tag = nltk.pos_tag(tokens)\n",
        "print(tag)\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN.*>}\"\n",
        "cp  =nltk.RegexpParser(grammar)\n",
        "result = cp.parse(tag)\n",
        "print(result)\n",
        "print(result.pformat_latex_qtree())\n",
        "result.pretty_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrY-bvpzIngC",
        "outputId": "85344dd3-9fa5-4440-942f-05cf94d696ad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'dog', 'chased', 'a', 'cat']\n",
            "[('A', 'DT'), ('dog', 'NN'), ('chased', 'VBD'), ('a', 'DT'), ('cat', 'NN')]\n",
            "(S (NP A/DT dog/NN) chased/VBD (NP a/DT cat/NN))\n",
            "\\Tree [.S [.NP A/DT dog/NN ] chased/VBD [.NP a/DT cat/NN ] ]\n",
            "                      S                   \n",
            "     _________________|__________          \n",
            "    |            NP              NP       \n",
            "    |        ____|____       ____|____     \n",
            "chased/VBD A/DT     dog/NN a/DT     cat/NN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 13. To show the function of FREDDIST PACKAGE IN NLTK\n",
        "from nltk import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk import FreqDist\n",
        "nltk.download('gutenberg')\n",
        "# Count each token in austen-persuasion.txt of the Gutenberg collection\n",
        "list_of_words = gutenberg.words(\"austen-persuasion.txt\")\n",
        "fd = FreqDist(list_of_words) # Frequency distribution object\n",
        "print(\"Total number of tokens: \" + str(fd.N())) # <insert_comment_how_many>\n",
        "print(\"Number of unique tokens: \" + str(fd.B())) # <insert_comment_how_many>\n",
        "print(\"Top 10 tokens:\") # <insert_comment_which_is_third token>\n",
        "for token, freq in fd.most_common(10):\n",
        "  print(token + \"\\t\" + str(freq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrXuD4g6KB9H",
        "outputId": "615da4c3-7f99-41a1-e05b-45e72815be07"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens: 98171\n",
            "Number of unique tokens: 6132\n",
            "Top 10 tokens:\n",
            ",\t6750\n",
            "the\t3120\n",
            "to\t2775\n",
            ".\t2741\n",
            "and\t2739\n",
            "of\t2564\n",
            "a\t1529\n",
            "in\t1346\n",
            "was\t1330\n",
            ";\t1290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14 RNN\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "x = layers.Input(shape=(12, 7))\n",
        "\n",
        "cell = layers.SimpleRNNCell(4, activation='tanh')\n",
        "\n",
        "rnn = layers.RNN(cell)\n",
        "\n",
        "output_rnn = rnn(x)\n",
        "\n",
        "output = layers.Dense(1, activation='sigmoid')(output_rnn)\n",
        "\n",
        "model = keras.Model(inputs=x, outputs=output)\n",
        "model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbitPsfdCbeG",
        "outputId": "e9824dd0-2643-4ddd-ce77-72cfeaf0bcb8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 12, 7)]           0         \n",
            "                                                                 \n",
            " rnn (RNN)                   (None, 4)                 48        \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53 (212.00 Byte)\n",
            "Trainable params: 53 (212.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15 LSTM\n",
        "!pip install -q tensorflow-datasets\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "# Load the IMDb reviews dataset\n",
        "dataset, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "# Take a subset of the datasets\n",
        "train_dataset = train_dataset.take(4000)\n",
        "test_dataset = test_dataset.take(1000)\n",
        "\n",
        "\n",
        "BUFFER_SIZE = 9000\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "VOCAB_SIZE = 1000\n",
        "encoder = layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))\n",
        "\n",
        "example = next(iter(train_dataset.take(1)))\n",
        "\n",
        "vocab = np.array(encoder.get_vocabulary())\n",
        "encoded_example = encoder(example[0]).numpy()\n",
        "\n",
        "# Display the length of the example and the encoded text\n",
        "print(\"Length of the example:\", len(example[0]))\n",
        "print(\"Encoded text:\", encoded_example)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFfvg2TyDxRd",
        "outputId": "8db38688-8bd5-490a-c8b5-0897387da872"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the example: 128\n",
            "Encoded text: [[ 35   1  52 ...   0   0   0]\n",
            " [  1   2   1 ...   0   0   0]\n",
            " [410   1   7 ...   0   0   0]\n",
            " ...\n",
            " [ 53   9  74 ...   0   0   0]\n",
            " [ 10   7   1 ...   0   0   0]\n",
            " [635   6   2 ...   0   0   0]]\n"
          ]
        }
      ]
    }
  ]
}