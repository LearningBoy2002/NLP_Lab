{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 1**"
      ],
      "metadata": {
        "id": "ABgUGi9du18W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kGz22ZLI_pef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb920e48-0bc5-4686-e971-4efa405b31eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good morning everyone.', 'Today we will study NLTK.']\n",
            "['Good', 'morning', 'everyone', '.', 'Today', 'we', 'will', 'study', 'NLTK', '.']\n",
            "Good morning everyone.\n",
            "Good\n",
            "morning\n",
            "everyone\n",
            ".\n",
            "Today we will study NLTK.\n",
            "Today\n",
            "we\n",
            "will\n",
            "study\n",
            "NLTK\n",
            ".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#1.To perfrome Tokenization of a given sentence\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "example=\"Good morning everyone. Today we will study NLTK.\"\n",
        "print(sent_tokenize(example))\n",
        "print(word_tokenize(example))\n",
        "for i in (sent_tokenize(example)):\n",
        "  print (i)\n",
        "  for j in (word_tokenize(i)):\n",
        "    print (j)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 2**"
      ],
      "metadata": {
        "id": "EORyJ1iVvKDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. To find the stopwords of a given text\n",
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words)\n",
        "words=word_tokenize(example)\n",
        "remove_stopwords=[]\n",
        "for i in words:\n",
        "  if i not in stop_words:\n",
        "    remove_stopwords.append(i)\n",
        "print(remove_stopwords)\n",
        "remove_words=[]\n",
        "remove_words= [i for i in words if i in stop_words]\n",
        "print(remove_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W51ZTO3vvREa",
        "outputId": "a458f3fa-d610-4604-fa31-a3e4b38380cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'whom', 'with', 'in', 'didn', \"didn't\", 'its', 'what', 'just', \"won't\", 'some', \"don't\", 'herself', 'on', 'should', 'theirs', 'about', 'them', 'again', 'here', 'we', \"doesn't\", \"hadn't\", 'weren', 'hers', 'itself', 'over', 'to', 'the', 'and', 'hasn', 'me', 'being', 'can', \"you've\", 'while', 'more', 'who', 'when', 'through', 'too', 'during', \"it's\", 'than', 'if', 'few', 'these', 'until', 'at', 'nor', 'because', 'why', 'very', 'his', 'm', 'wouldn', 'won', 'o', 'an', 'haven', 'am', 'those', \"mustn't\", 'it', 'which', 'they', 'don', \"that'll\", 'own', 'd', 'ain', \"you're\", 'this', 'shouldn', \"she's\", 'for', \"isn't\", 'll', 'had', \"weren't\", 'he', 's', 'himself', 'mightn', 'isn', 'ma', 'did', 'i', 'not', \"you'll\", 'how', 'any', 'him', 'our', \"you'd\", \"couldn't\", 'hadn', 'needn', 'will', 'shan', \"should've\", 'up', 'most', 'wasn', 'she', 'yourself', 'from', 'that', 'their', 'between', 'themselves', 'once', 'all', 'were', 'below', 'is', 'into', 'yours', 'couldn', 'do', 'before', \"shan't\", 're', 'aren', 'are', 'after', 'above', 'has', 'only', 'now', \"needn't\", \"wouldn't\", \"wasn't\", 'your', 'against', 'off', \"mightn't\", 'further', 'doing', 'then', 'or', 'ours', 'was', 've', 'yourselves', 'been', 'ourselves', 'have', 'having', 'each', 'so', 'y', 'down', 'but', 'does', 'my', 'no', 'mustn', 'as', 'where', \"hasn't\", 'out', \"shouldn't\", 'by', 't', 'you', 'both', 'her', 'other', \"haven't\", 'a', 'there', 'myself', 'doesn', 'under', 'of', 'be', 'same', \"aren't\", 'such'}\n",
            "['Good', 'morning', 'everyone', '.', 'Today', 'study', 'NLTK', '.']\n",
            "['we', 'will']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 3**"
      ],
      "metadata": {
        "id": "HSwOfW4Zw0zV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3. To perform stemming operation using NLTK\n",
        "import nltk.stem\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "ps=PorterStemmer()\n",
        "sb=SnowballStemmer((\"english\"))\n",
        "words= [\"Computer\",\"Computerization\",\"Computerize\"]\n",
        "for i in words:\n",
        "  print(i+\":\" +ps.stem(i))\n",
        "print(\"---------\")\n",
        "for i in words:\n",
        "  print(i+\":\" +sb.stem(i))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfMmEw58wOEh",
        "outputId": "658fe092-eec3-4c4a-e120-135d53e99075"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computer:comput\n",
            "Computerization:computer\n",
            "Computerize:computer\n",
            "---------\n",
            "Computer:comput\n",
            "Computerization:computer\n",
            "Computerize:computer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 4**"
      ],
      "metadata": {
        "id": "4gmgK90XxOks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. To perfrom POS tagging of a given text\n",
        "import nltk.stem\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download('state_union')\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "text= state_union.raw(\"2005-GWBush.txt\")\n",
        "text=\"Good morning everyone. Today we will study NLTK.\"\n",
        "custom_sent_tokenizer1= sent_tokenize(text)\n",
        "print(custom_sent_tokenizer1)\n",
        "custom_sent_tokenizer= PunktSentenceTokenizer(text)\n",
        "tokenized= custom_sent_tokenizer.tokenize(text)\n",
        "print(tokenized)\n",
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized:\n",
        "      words= nltk.word_tokenize(i)\n",
        "      tagged= nltk.pos_tag(words)\n",
        "      print(tagged)\n",
        "  except Exception as e:\n",
        "   print(str(e))\n",
        "process_content()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mw8d45nxBqD",
        "outputId": "154381f2-c55f-4dba-b919-c26bbb9f24eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good morning everyone.', 'Today we will study NLTK.']\n",
            "['Good morning everyone.', 'Today we will study NLTK.']\n",
            "[('Good', 'JJ'), ('morning', 'NN'), ('everyone', 'NN'), ('.', '.')]\n",
            "[('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('study', 'VB'), ('NLTK', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 5**"
      ],
      "metadata": {
        "id": "G4mSIqFxxl4s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7h0-b4JdDpdC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Take two sentence from 'austen-emma.text' and then perfrom stemming\n",
        "import nltk\n",
        "import requests\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Corrected URL\n",
        "url = \"https://raw.githubusercontent.com/fbkarsdorp/Python-Course/master/data/austen-emma.txt\"\n",
        "\n",
        "response = requests.get(url)\n",
        "text = response.text  # Use .text attribute to get the content\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Tokenizing sentences\n",
        "tokenized_sentences = sent_tokenize(text)\n",
        "\n",
        "# Selecting specific sentences\n",
        "selected_sentences = tokenized_sentences[3:5]\n",
        "\n",
        "for sentence in selected_sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    stemmed_words = [ps.stem(word) for word in words]\n",
        "    stemmed_sentence = ' '.join(stemmed_words)\n",
        "    print(stemmed_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9Yl4u0CxgtX",
        "outputId": "af31678a-24e1-4444-a905-c9b93d4c8b8f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sixteen year had miss taylor been in mr. woodhous 's famili , less as a gover than a friend , veri fond of both daughter , but particularli of emma .\n",
            "between _them_ it wa more the intimaci of sister .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Take two sentence then perfrom stemming\n",
        "import nltk.stem\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "ps=PorterStemmer()\n",
        "sb=SnowballStemmer((\"english\"))\n",
        "words= [\"Computer\",\"Computerization\",\"Computerize\"]\n",
        "for i in words:\n",
        "  print(i+\":\" +ps.stem(i))\n",
        "print(\"---------\")\n",
        "for i in words:\n",
        "  print(i+\":\" +sb.stem(i))"
      ],
      "metadata": {
        "id": "7v2TQZ83CCAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6e224b-c632-443c-f3fb-c782fe76d3df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computer:comput\n",
            "Computerization:computer\n",
            "Computerize:computer\n",
            "---------\n",
            "Computer:comput\n",
            "Computerization:computer\n",
            "Computerize:computer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6,11 Ngram in given text or sentence\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#import bigrams, trigrams, ngrams\n",
        "text=\"Good morning everyone. Today we will study NLTK. Good afternoon everyone.\"\n",
        "tokens=nltk.word_tokenize(text)\n",
        "# bigrams_list=list(nltk.bigrams(tokens))\n",
        "# print(bigrams_list)\n",
        "\n",
        "ngrams_list=list(nltk.ngrams(tokens,3))##\n",
        "print(ngrams_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xei8FKQyDxrd",
        "outputId": "e61323b1-6ba6-4683-d5c0-c5cb175bc589"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Good', 'morning', 'everyone'), ('morning', 'everyone', '.'), ('everyone', '.', 'Today'), ('.', 'Today', 'we'), ('Today', 'we', 'will'), ('we', 'will', 'study'), ('will', 'study', 'NLTK'), ('study', 'NLTK', '.'), ('NLTK', '.', 'Good'), ('.', 'Good', 'afternoon'), ('Good', 'afternoon', 'everyone'), ('afternoon', 'everyone', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.  To perfrom  Tokenization and POS tagging from 'austen-emma.text'\n",
        "# import nltk.stem\n",
        "# from nltk.corpus import state_union\n",
        "# from nltk.tokenize import sent_tokenize\n",
        "# nltk.download(\"averaged_perceptron_tagger\")\n",
        "# nltk.download('state_union')\n",
        "# from nltk.tokenize import PunktSentenceTokenizer\n",
        "# text= state_union.raw(\"2005-GWBush.txt\")\n",
        "# text=\"Good morning everyone. Today we will study NLTK.\"\n",
        "# custom_sent_tokenizer1= sent_tokenize(text)\n",
        "# print(custom_sent_tokenizer1)\n",
        "# custom_sent_tokenizer= PunktSentenceTokenizer(text)\n",
        "# tokenized= custom_sent_tokenizer.tokenize(text)\n",
        "# print(tokenized)\n",
        "# def process_content():\n",
        "#   try:\n",
        "#     for i in tokenized:\n",
        "#       words= nltk.word_tokenize(i)\n",
        "#       tagged= nltk.pos_tag(words)\n",
        "#       print(tagged)\n",
        "#   except Exception as e:\n",
        "#    print(str(e))\n",
        "# process_content()\n"
      ],
      "metadata": {
        "id": "Wx7ANAtEET1V"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. To perfrom the named Entity recognition using the chunking method\n",
        "from nltk import ne_chunk\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "train_text=\" Sundaram Pichai is a CEO of GOOGLE Company\"\n",
        "sample_text=\" Sundaram Pichai is a CEO of GOOGLE Company\"\n",
        "custom_sent_tokenizer= PunktSentenceTokenizer(train_text)\n",
        "tokenized= custom_sent_tokenizer.tokenize(sample_text)\n",
        "print(tokenized)\n",
        "def process_content():\n",
        "  try:\n",
        "     for i in tokenized:\n",
        "        words= nltk.word_tokenize(i)\n",
        "        tagged= nltk.pos_tag(words)#('averaged perceptron tagger')\n",
        "        chunks=nltk.ne_chunk(tagged)\n",
        "        print(chunks)\n",
        "        chunks.pretty_print(unicodelines=True)\n",
        "  except Exception as e:\n",
        "        print(str(e))\n",
        "process_content()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzOBwQ5UIkKF",
        "outputId": "56ca652a-e568-46de-ff4f-8cccd73b7c91"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' Sundaram Pichai is a CEO of GOOGLE Company']\n",
            "(S\n",
            "  (PERSON Sundaram/NNP)\n",
            "  (PERSON Pichai/NNP)\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  CEO/NN\n",
            "  of/IN\n",
            "  (ORGANIZATION GOOGLE/NNP Company/NNP))\n",
            "                     S                                                              \n",
            "  ┌─────┬─────┬──────┼────────┬───────────┬──────────────────────┐                   \n",
            "  │     │     │      │      PERSON      PERSON              ORGANIZATION            \n",
            "  │     │     │      │        │           │          ┌───────────┴────────────┐      \n",
            "is/VBZ a/DT CEO/NN of/IN Sundaram/NNP Pichai/NNP GOOGLE/NNP              Company/NNP\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. to perfrom Chuunking operation\n",
        "from sklearn import tree\n",
        "from nltk.tree import *\n",
        "text = \"A dog chased a cat\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)\n",
        "tag = nltk.pos_tag(tokens)\n",
        "print(tag)\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN.*>}\"\n",
        "cp  =nltk.RegexpParser(grammar)\n",
        "result = cp.parse(tag)\n",
        "print(result)\n",
        "print(result.pformat_latex_qtree())\n",
        "result.pretty_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrY-bvpzIngC",
        "outputId": "144bdb17-3d8b-4c5c-d7d8-2ff6efb03e39"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'dog', 'chased', 'a', 'cat']\n",
            "[('A', 'DT'), ('dog', 'NN'), ('chased', 'VBD'), ('a', 'DT'), ('cat', 'NN')]\n",
            "(S (NP A/DT dog/NN) chased/VBD (NP a/DT cat/NN))\n",
            "\\Tree [.S [.NP A/DT dog/NN ] chased/VBD [.NP a/DT cat/NN ] ]\n",
            "                      S                   \n",
            "     _________________|__________          \n",
            "    |            NP              NP       \n",
            "    |        ____|____       ____|____     \n",
            "chased/VBD A/DT     dog/NN a/DT     cat/NN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 13. To show the function of FREDDIST PACKAGE IN NLTK\n",
        "from nltk import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "# data = \"All work and no play makes jack dull boy. All work and no play makes jack \"\n",
        "# words = word_tokenize(data)\n",
        "# fdist1 = FreqDist(words)\n",
        "# print(fdist1.most_common(2)) # Prints two most common tokens\n",
        "# print(fdist1.hapaxes()) # Prints tokens with frequency 1\n",
        "# [('All', 2), ('work', 2)]\n",
        "# ['a']\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk import FreqDist\n",
        "nltk.download('gutenberg')\n",
        "# Count each token in austen-persuasion.txt of the Gutenberg collection\n",
        "list_of_words = gutenberg.words(\"austen-persuasion.txt\")\n",
        "fd = FreqDist(list_of_words) # Frequency distribution object\n",
        "print(\"Total number of tokens: \" + str(fd.N())) # <insert_comment_how_many>\n",
        "print(\"Number of unique tokens: \" + str(fd.B())) # <insert_comment_how_many>\n",
        "print(\"Top 10 tokens:\") # <insert_comment_which_is_third token>\n",
        "for token, freq in fd.most_common(10):\n",
        "  print(token + \"\\t\" + str(freq))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrXuD4g6KB9H",
        "outputId": "67b1ad66-a80c-4046-b6a9-1b520808aa45"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens: 98171\n",
            "Number of unique tokens: 6132\n",
            "Top 10 tokens:\n",
            ",\t6750\n",
            "the\t3120\n",
            "to\t2775\n",
            ".\t2741\n",
            "and\t2739\n",
            "of\t2564\n",
            "a\t1529\n",
            "in\t1346\n",
            "was\t1330\n",
            ";\t1290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oar5RkRaKw4f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}