{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VB86SF6ZP71"
      },
      "source": [
        "**Experiment 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdyHsHiDX_SK",
        "outputId": "2b45eb24-6d01-4ff3-e4cf-a307a9e8a8de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeBjS0EwX-oK",
        "outputId": "d16577e1-2933-4d36-ceb2-4bbf622bf882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Good Morning Everyone.', 'Today we will study NLTK.']\n",
            "['Good', 'Morning', 'Everyone', '.', 'Today', 'we', 'will', 'study', 'NLTK', '.']\n",
            "Good Morning Everyone.\n",
            "Good\n",
            "Morning\n",
            "Everyone\n",
            ".\n",
            "Today we will study NLTK.\n",
            "Today\n",
            "we\n",
            "will\n",
            "study\n",
            "NLTK\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "example = \"Good Morning Everyone. Today we will study NLTK.\"\n",
        "print(sent_tokenize(example))\n",
        "print(word_tokenize(example))\n",
        "for i in (sent_tokenize(example)):\n",
        "  print(i)\n",
        "  for j in (word_tokenize(i)):\n",
        "    print(j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j81AVafZYB2"
      },
      "source": [
        "**Experiment 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPce2Bizdqf8"
      },
      "source": [
        "Practice Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO9k_3rYdv99",
        "outputId": "e41f4b05-7cfe-494a-bf29-8f1af22dc1c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRP9bNvFd13e",
        "outputId": "1287201f-9972-4f06-a59b-045e03238ad2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVvdkURteA6e"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBhiMm-ueCw-",
        "outputId": "5eac4bb2-1462-41a0-b0c9-628c9fda5161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method CategorizedCorpusReader.words of <CategorizedTaggedCorpusReader in '/root/nltk_data/corpora/brown'>>\n"
          ]
        }
      ],
      "source": [
        "print(brown.words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "8q_3088DeI1b",
        "outputId": "6a690b8a-592d-426d-900a-21f48b429a6a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-e6f8b21259d5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'brown' is not defined"
          ]
        }
      ],
      "source": [
        "brown.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywdIvvHQeS77"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebOKDc3YebXE",
        "outputId": "261620f5-51e3-400f-e14a-fbd98649f25c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImOtRWv1eg_q"
      },
      "source": [
        "Question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cW84HZamYUYD"
      },
      "outputs": [],
      "source": [
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQIDfdE6aGYN",
        "outputId": "429038fe-cf2e-4efa-c7c1-b323adcbd3a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAt1WaYzcPYG"
      },
      "outputs": [],
      "source": [
        "stop_words=set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Mgd3chcQpF",
        "outputId": "f3e33602-1a5a-4553-9682-09aafafd0bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'some', 'if', 'by', 'had', 'ain', \"she's\", 'its', 'own', 'above', 'once', 'we', 'hadn', \"mightn't\", 'needn', 'shan', 'both', 'this', 'mustn', 'while', 'down', 'now', \"isn't\", 'for', 'most', 'it', 'were', 'so', 'does', 'below', 'who', 'are', \"shouldn't\", 'to', 'all', 'same', \"mustn't\", 'over', \"you're\", 'having', 're', \"didn't\", 'm', 'until', 'as', 'again', 'before', 'he', 'but', 'themselves', 'will', 'with', 'only', 'aren', \"you'd\", 'few', 'such', 'isn', 'here', 'other', 'any', 'not', \"that'll\", 'wasn', 'i', 'itself', 's', 'how', 'because', 'being', 'then', 'no', 'each', 'should', \"doesn't\", 'off', 'them', 'doing', 'been', 'ma', 'in', 'a', 'him', 'that', 'after', 'into', \"it's\", 've', 'my', 'shouldn', 'have', 'you', 'between', 'from', 'or', 'very', 'o', 'whom', \"couldn't\", \"hadn't\", 'didn', 'further', 'what', 't', 'she', 'there', \"should've\", \"haven't\", 'ours', 'll', 'myself', 'when', \"hasn't\", \"you've\", 'during', 'hers', 'and', 'haven', 'their', 'out', 'don', 'herself', 'of', 'against', 'did', \"you'll\", 'under', 'am', 'has', 'yourself', \"won't\", \"aren't\", 'those', 'y', 'about', 'where', 'on', 'yourselves', 'do', 'your', 'just', 'd', 'wouldn', \"wouldn't\", 'our', 'was', \"needn't\", 'through', 'too', 'can', 'which', 'nor', \"wasn't\", 'an', 'ourselves', 'up', \"don't\", 'weren', 'her', 'mightn', 'is', 'himself', 'theirs', 'they', 'at', 'couldn', 'me', 'hasn', 'won', 'why', 'his', 'than', 'be', \"weren't\", 'the', 'these', 'doesn', 'more', \"shan't\", 'yours'}\n"
          ]
        }
      ],
      "source": [
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEN9uCb8abbO",
        "outputId": "29d9c47f-90e8-4853-a599-76e3f06aa634"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Good']\n",
            "['we', 'will']\n",
            "['Good', 'Morning']\n",
            "['we', 'will']\n",
            "['Good', 'Morning', 'Everyone']\n",
            "['we', 'will']\n",
            "['Good', 'Morning', 'Everyone', '.']\n",
            "['we', 'will']\n",
            "['Good', 'Morning', 'Everyone', '.', 'Today']\n",
            "['we', 'will']\n",
            "['Good', 'Morning', 'Everyone', '.', 'Today']\n",
            "['we', 'will']\n",
            "['Good', 'Morning', 'Everyone', '.', 'Today']\n",
            "['we', 'will']\n",
            "['Good', 'Morning', 'Everyone', '.', 'Today', 'study']\n",
            "['we', 'will']\n",
            "['Good', 'Morning', 'Everyone', '.', 'Today', 'study', 'NLTK']\n",
            "['we', 'will']\n",
            "['Good', 'Morning', 'Everyone', '.', 'Today', 'study', 'NLTK', '.']\n",
            "['we', 'will']\n"
          ]
        }
      ],
      "source": [
        "example='Good Morning Everyone. Today we will study NLTK.'\n",
        "words=word_tokenize(example)\n",
        "remove_stopwords=[]\n",
        "for i in words:\n",
        "  if i not in stop_words:\n",
        "    remove_stopwords.append(i)\n",
        "  print(remove_stopwords)\n",
        "  remove_words=[]\n",
        "  remove_words=[i for i in words if i in stop_words]\n",
        "  print(remove_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6jbuRE2eqYQ"
      },
      "source": [
        "**Experiment 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ovvf0-ygaS5B"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjJzyhyJfBCS"
      },
      "outputs": [],
      "source": [
        "ps=PorterStemmer()\n",
        "sb=SnowballStemmer(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoMkeyvbf5Ax"
      },
      "outputs": [],
      "source": [
        "example='Good Morning Everyone. Today we will study NLTK.'\n",
        "words=word_tokenize(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BEfGRlKfSM6"
      },
      "outputs": [],
      "source": [
        "words=(words + ['Computer','Computerization','Computerize'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeCdfn9AgUEh",
        "outputId": "fab260a9-c801-4838-9168-a432f6818e0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good:good\n",
            "Morning:morn\n",
            "Everyone:everyon\n",
            ".:.\n",
            "Today:today\n",
            "we:we\n",
            "will:will\n",
            "study:studi\n",
            "NLTK:nltk\n",
            ".:.\n",
            "Computer:comput\n",
            "Computerization:computer\n",
            "Computerize:computer\n"
          ]
        }
      ],
      "source": [
        "for i in words:\n",
        "  print(i+ \":\"  + ps.stem(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOa2TLxIjldk",
        "outputId": "59256540-536f-4a9c-8607-b5bd62a5cbcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------\n"
          ]
        }
      ],
      "source": [
        "print(\"------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX3iK9SPjp27",
        "outputId": "75af226c-b2a7-4cf7-d717-c4e48de50301"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good:good\n",
            "Morning:morn\n",
            "Everyone:everyon\n",
            ".:.\n",
            "Today:today\n",
            "we:we\n",
            "will:will\n",
            "study:studi\n",
            "NLTK:nltk\n",
            ".:.\n",
            "Computer:comput\n",
            "Computerization:computer\n",
            "Computerize:computer\n"
          ]
        }
      ],
      "source": [
        "for i in words:\n",
        "  print(i+\":\"+sb.stem(i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXdA5GY2jvcB"
      },
      "source": [
        "**Experiment 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v657WGpjuf8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN9hnsp-kcvx",
        "outputId": "290a59df-cd5e-48ea-8c94-9ad8809cf35b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading average_perceptron_tagger: Package\n",
            "[nltk_data]     'average_perceptron_tagger' not found in index\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('average_perceptron_tagger')\n",
        "nltk.download('state_union')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv0HlAvjkwYa",
        "outputId": "2f056769-f772-44f5-c82a-84a3e6a4c052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Good Morning Everone.', 'Today we will study NLTK.']\n",
            "['Good Morning Everone.', 'Today we will study NLTK.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "text='Good Morning Everone. Today we will study NLTK.'\n",
        "custom_sent_tokenizer1=sent_tokenize(text)\n",
        "print(custom_sent_tokenizer1)\n",
        "custom_sent_tokenizer=PunktSentenceTokenizer(text)\n",
        "tokenized=custom_sent_tokenizer.tokenize(text)\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR6KfjeTleMX",
        "outputId": "c19b4c41-d98c-441d-d883-9f6727da9dd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized:\n",
        "      words=nltk.word_tokenize(i)\n",
        "      tagged=nltk.pos_tag(words)\n",
        "    print(tagged)\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "process_content()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HwyD0LK0hL-",
        "outputId": "b8e822cc-a9b6-43d6-c005-6504826b64cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1: if you were saying something at Ford ' s carriage.\n",
            "Stemmed Sentence 1: if you were say someth at ford ' s carriag . \n",
            "\n",
            "Sentence 2: Jane was wanting to make Emma want their advice ; and when he takes her from hearing it bandied between the other day .\n",
            "Stemmed Sentence 2: jane wa want to make emma want their advic ; and when he take her from hear it bandi between the other day .\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample sentences\n",
        "sentence1 = \"if you were saying something at Ford ' s carriage.\"\n",
        "sentence2 = \"Jane was wanting to make Emma want their advice ; and when he takes her from hearing it bandied between the other day .\"\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Tokenize and stem the sentences\n",
        "tokens1 = word_tokenize(sentence1)\n",
        "stemmed_sentence1 = \" \".join([stemmer.stem(word) for word in tokens1])\n",
        "\n",
        "tokens2 = word_tokenize(sentence2)\n",
        "stemmed_sentence2 = \" \".join([stemmer.stem(word) for word in tokens2])\n",
        "\n",
        "# Print the stemmed sentences\n",
        "print(\"Sentence 1:\",sentence1)\n",
        "print(\"Stemmed Sentence 1:\", stemmed_sentence1,'\\n')\n",
        "print(\"Sentence 2:\",sentence2)\n",
        "print(\"Stemmed Sentence 2:\", stemmed_sentence2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "tTvpFMuqDImr",
        "outputId": "9f757812-6c6b-427a-ff4a-cc067b2da9cc"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-521f34b95d55>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    file_path = \"C:\\Users\\dell\\Documents\"\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "xAa0XAqfD4em",
        "outputId": "a70395c5-f33e-475e-c0fe-42622cc9c9a2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-03a7794e-55fa-4607-a9f9-c0c9d0e980fd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-03a7794e-55fa-4607-a9f9-c0c9d0e980fd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving ngrams_nlp.txt to ngrams_nlp.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3-63o9iD5OM",
        "outputId": "5acba71b-302e-45dd-dbd3-83f82108dbb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: Hello.\n",
            "2-grams: [('Hello', '.')]\n",
            "\n",
            "Sentence: I'm Dipa Dutta.This is my Lab Assignment to find ngrams of each sentence from a paragraph taken from user defined file.I'll be using Google Colab for the same.\n",
            "2-grams: [('I', \"'m\"), (\"'m\", 'Dipa'), ('Dipa', 'Dutta.This'), ('Dutta.This', 'is'), ('is', 'my'), ('my', 'Lab'), ('Lab', 'Assignment'), ('Assignment', 'to'), ('to', 'find'), ('find', 'ngrams'), ('ngrams', 'of'), ('of', 'each'), ('each', 'sentence'), ('sentence', 'from'), ('from', 'a'), ('a', 'paragraph'), ('paragraph', 'taken'), ('taken', 'from'), ('from', 'user'), ('user', 'defined'), ('defined', 'file.I'), ('file.I', \"'ll\"), (\"'ll\", 'be'), ('be', 'using'), ('using', 'Google'), ('Google', 'Colab'), ('Colab', 'for'), ('for', 'the'), ('the', 'same'), ('same', '.')]\n",
            "\n",
            "Sentence: Hey!\n",
            "2-grams: [('Hey', '!')]\n",
            "\n",
            "Sentence: !,do you know ICC Men's World Cup is going on and India has secured the topmost position in the table till now.\n",
            "2-grams: [('!', ','), (',', 'do'), ('do', 'you'), ('you', 'know'), ('know', 'ICC'), ('ICC', 'Men'), ('Men', \"'s\"), (\"'s\", 'World'), ('World', 'Cup'), ('Cup', 'is'), ('is', 'going'), ('going', 'on'), ('on', 'and'), ('and', 'India'), ('India', 'has'), ('has', 'secured'), ('secured', 'the'), ('the', 'topmost'), ('topmost', 'position'), ('position', 'in'), ('in', 'the'), ('the', 'table'), ('table', 'till'), ('till', 'now'), ('now', '.')]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import ngrams\n",
        "\n",
        "# Read the text from the user-defined file\n",
        "file_path = \"ngrams_nlp.txt\"\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "# Define a function to generate n-grams\n",
        "def get_ngrams(sentence, n):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return list(ngrams(words, n))\n",
        "\n",
        "# Specify the value of 'n' for your desired n-grams\n",
        "n = 2  # Change this to the desired n-gram size\n",
        "\n",
        "# Find and print n-grams for each sentence\n",
        "for sentence in sentences:\n",
        "    n_grams = get_ngrams(sentence, n)\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"{n}-grams: {n_grams}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MpelhMxKw0q",
        "outputId": "404da5b7-5d70-4ef6-d5bc-30fb499e7757"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "T564rV9nNUO3",
        "outputId": "e3a76942-7794-4865-ecbe-0633db08972c"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-d340e6ae1a12>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from gutenberg.fileids import austen-emma.txt\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "from gutenberg.fileids import austen-emma.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "kEJP0WZVNGlK",
        "outputId": "a892ce53-7ace-4545-fb04-43acab5d3bdf"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d1282c7ce0fc>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Read the text from 'austen-emma.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'austen-emma.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'austen-emma.txt'"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Read the text from 'austen-emma.txt'\n",
        "with open('austen-emma.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Tokenize each sentence into words and perform POS tagging\n",
        "tagged_sentences = [pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
        "\n",
        "# Example: Printing the first sentence with POS tags\n",
        "print(\"Tokenized and POS-tagged sentences:\")\n",
        "for word, pos in tagged_sentences[0]:\n",
        "    print(f\"{word} - {pos}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbARxpXQNJIn"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsZQAesDO_hL",
        "outputId": "4bb9e2b6-f390-43a5-d792-03e00751a7e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCRV-g78PAJY"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWhNIIjKPUqg",
        "outputId": "22bbfeee-6e05-4b9d-f827-232169474b70"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9us6VID_RERV",
        "outputId": "0dd97170-f061-468b-cb24-1acc7f205637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Absolute path: /content/austen-emma.txt\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import gutenberg\n",
        "file_ids = gutenberg.fileids()\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "file_name = \"austen-emma.txt\"  # Replace with the name of your file\n",
        "absolute_path = os.path.abspath(file_name)\n",
        "\n",
        "print(\"Absolute path:\", absolute_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "YzFEIM2yPVNF",
        "outputId": "3e2f3df4-2601-4dbf-f89f-30958eca43fc"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-f90b5eb0687b>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#file_path = \"/content/austen-emma.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsolute_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/austen-emma.txt'"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Read the text from 'austen-emma.txt'\n",
        "#chosen_file_id = \"/content/austen-emma.txt\"  # Replace with the file ID you want to import\n",
        "#text = gutenberg.raw(chosen_file_id)\n",
        "#with open(\"austen-emma.txt\", 'r', encoding='utf-8') as file:\n",
        "    #text = file.read()\n",
        "\n",
        "#file_path = \"/content/austen-emma.txt\"\n",
        "with open(absolute_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Tokenize each sentence into words and perform POS tagging\n",
        "tagged_sentences = [pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
        "\n",
        "# Example: Printing the first sentence with POS tags\n",
        "print(\"Tokenized and POS-tagged sentences:\")\n",
        "for word, pos in tagged_sentences[0]:\n",
        "    print(f\"{word} - {pos}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2y3Hjv5PXdI",
        "outputId": "742fab4d-b937-4949-aaa8-84e45b3b0d12"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOIPE2uWf5HY"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Load the text from \"austen-emma.txt\" in the Gutenberg corpus\n",
        "emma_text = gutenberg.raw('austen-emma.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBCkIaVWf8Ui"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(emma_text)\n",
        "\n",
        "# Tokenize each sentence into words\n",
        "word_tokens = [word_tokenize(sentence) for sentence in sentences]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnWZW2jXgAFa",
        "outputId": "013f0e9c-8359-411f-8c14-f7c8478ede5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POS tags for the first sentence:\n",
            "[('[', 'NNS'), ('Emma', 'NNP'), ('by', 'IN'), ('Jane', 'NNP'), ('Austen', 'NNP'), ('1816', 'CD'), (']', 'NNP'), ('VOLUME', 'NNP'), ('I', 'PRP'), ('CHAPTER', 'VBP'), ('I', 'PRP'), ('Emma', 'NNP'), ('Woodhouse', 'NNP'), (',', ','), ('handsome', 'NN'), (',', ','), ('clever', 'NN'), (',', ','), ('and', 'CC'), ('rich', 'JJ'), (',', ','), ('with', 'IN'), ('a', 'DT'), ('comfortable', 'JJ'), ('home', 'NN'), ('and', 'CC'), ('happy', 'JJ'), ('disposition', 'NN'), (',', ','), ('seemed', 'VBD'), ('to', 'TO'), ('unite', 'VB'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('blessings', 'NNS'), ('of', 'IN'), ('existence', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), ('lived', 'VBN'), ('nearly', 'RB'), ('twenty-one', 'CD'), ('years', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('with', 'IN'), ('very', 'RB'), ('little', 'JJ'), ('to', 'TO'), ('distress', 'VB'), ('or', 'CC'), ('vex', 'VB'), ('her', 'PRP'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "# Perform POS tagging on the word tokens\n",
        "pos_tags = [pos_tag(tokens) for tokens in word_tokens]\n",
        "\n",
        "# Print the POS tags for the first sentence\n",
        "print(\"POS tags for the first sentence:\")\n",
        "print(pos_tags[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1fkdzvFgiWH",
        "outputId": "616701bd-98f0-4818-d4a0-da803a7113fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/porter_test.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('porter_test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLIMXye1gjF4"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Load the text from \"austen-emma.txt\" in the Gutenberg corpus\n",
        "emma_text = gutenberg.raw('austen-emma.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjSpZGlxglZ-"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(emma_text)\n",
        "\n",
        "# Select any two sentences (e.g., first and second sentences)\n",
        "selected_sentences = sentences[:2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1CWyOIpgn0p",
        "outputId": "07f368d3-0f4a-4d16-c951-777e243c33fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.', \"She was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.\"]\n",
            "Stemmed sentence 1:\n",
            "[ emma by jane austen 1816 ] volum i chapter i emma woodhous , handsom , clever , and rich , with a comfort home and happi disposit , seem to unit some of the best bless of exist ; and had live nearli twenty-on year in the world with veri littl to distress or vex her .\n",
            "Stemmed sentence 2:\n",
            "she wa the youngest of the two daughter of a most affection , indulg father ; and had , in consequ of her sister 's marriag , been mistress of hi hous from a veri earli period .\n"
          ]
        }
      ],
      "source": [
        "# Create a stemming function using the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Perform stemming on the selected sentences\n",
        "stemmed_sentences = []\n",
        "\n",
        "for sentence in selected_sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    stemmed_sentence = ' '.join(stemmed_words)\n",
        "    stemmed_sentences.append(stemmed_sentence)\n",
        "\n",
        "print(sentences[:2])\n",
        "\n",
        "# Print the stemmed sentences\n",
        "for i, stemmed_sentence in enumerate(stemmed_sentences):\n",
        "    print(f\"Stemmed sentence {i + 1}:\")\n",
        "    print(stemmed_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Iy9EXo1ERwI",
        "outputId": "ae3bdf1c-77bb-46fd-e677-b62356b833c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gMYGO7pgqPj",
        "outputId": "5b1d02f4-624a-4b89-ba18-397c81df6383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(: opening parenthesis\n",
            "    (\n",
            "): closing parenthesis\n",
            "    )\n",
            "*: negator\n",
            "    not n't\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ? ; ! :\n",
            ":: colon\n",
            "    :\n",
            "ABL: determiner/pronoun, pre-qualifier\n",
            "    quite such rather\n",
            "ABN: determiner/pronoun, pre-quantifier\n",
            "    all half many nary\n",
            "ABX: determiner/pronoun, double conjunction or pre-quantifier\n",
            "    both\n",
            "AP: determiner/pronoun, post-determiner\n",
            "    many other next more last former little several enough most least only\n",
            "    very few fewer past same Last latter less single plenty 'nough lesser\n",
            "    certain various manye next-to-last particular final previous present\n",
            "    nuf\n",
            "AP$: determiner/pronoun, post-determiner, genitive\n",
            "    other's\n",
            "AP+AP: determiner/pronoun, post-determiner, hyphenated pair\n",
            "    many-much\n",
            "AT: article\n",
            "    the an no a every th' ever' ye\n",
            "BE: verb 'to be', infinitive or imperative\n",
            "    be\n",
            "BED: verb 'to be', past tense, 2nd person singular or all persons plural\n",
            "    were\n",
            "BED*: verb 'to be', past tense, 2nd person singular or all persons plural, negated\n",
            "    weren't\n",
            "BEDZ: verb 'to be', past tense, 1st and 3rd person singular\n",
            "    was\n",
            "BEDZ*: verb 'to be', past tense, 1st and 3rd person singular, negated\n",
            "    wasn't\n",
            "BEG: verb 'to be', present participle or gerund\n",
            "    being\n",
            "BEM: verb 'to be', present tense, 1st person singular\n",
            "    am\n",
            "BEM*: verb 'to be', present tense, 1st person singular, negated\n",
            "    ain't\n",
            "BEN: verb 'to be', past participle\n",
            "    been\n",
            "BER: verb 'to be', present tense, 2nd person singular or all persons plural\n",
            "    are art\n",
            "BER*: verb 'to be', present tense, 2nd person singular or all persons plural, negated\n",
            "    aren't ain't\n",
            "BEZ: verb 'to be', present tense, 3rd person singular\n",
            "    is\n",
            "BEZ*: verb 'to be', present tense, 3rd person singular, negated\n",
            "    isn't ain't\n",
            "CC: conjunction, coordinating\n",
            "    and or but plus & either neither nor yet 'n' and/or minus an'\n",
            "CD: numeral, cardinal\n",
            "    two one 1 four 2 1913 71 74 637 1937 8 five three million 87-31 29-5\n",
            "    seven 1,119 fifty-three 7.5 billion hundred 125,000 1,700 60 100 six\n",
            "    ...\n",
            "CD$: numeral, cardinal, genitive\n",
            "    1960's 1961's .404's\n",
            "CS: conjunction, subordinating\n",
            "    that as after whether before while like because if since for than altho\n",
            "    until so unless though providing once lest s'posin' till whereas\n",
            "    whereupon supposing tho' albeit then so's 'fore\n",
            "DO: verb 'to do', uninflected present tense, infinitive or imperative\n",
            "    do dost\n",
            "DO*: verb 'to do', uninflected present tense or imperative, negated\n",
            "    don't\n",
            "DO+PPSS: verb 'to do', past or present tense + pronoun, personal, nominative, not 3rd person singular\n",
            "    d'you\n",
            "DOD: verb 'to do', past tense\n",
            "    did done\n",
            "DOD*: verb 'to do', past tense, negated\n",
            "    didn't\n",
            "DOZ: verb 'to do', present tense, 3rd person singular\n",
            "    does\n",
            "DOZ*: verb 'to do', present tense, 3rd person singular, negated\n",
            "    doesn't don't\n",
            "DT: determiner/pronoun, singular\n",
            "    this each another that 'nother\n",
            "DT$: determiner/pronoun, singular, genitive\n",
            "    another's\n",
            "DT+BEZ: determiner/pronoun + verb 'to be', present tense, 3rd person singular\n",
            "    that's\n",
            "DT+MD: determiner/pronoun + modal auxillary\n",
            "    that'll this'll\n",
            "DTI: determiner/pronoun, singular or plural\n",
            "    any some\n",
            "DTS: determiner/pronoun, plural\n",
            "    these those them\n",
            "DTS+BEZ: pronoun, plural + verb 'to be', present tense, 3rd person singular\n",
            "    them's\n",
            "DTX: determiner, pronoun or double conjunction\n",
            "    neither either one\n",
            "EX: existential there\n",
            "    there\n",
            "EX+BEZ: existential there + verb 'to be', present tense, 3rd person singular\n",
            "    there's\n",
            "EX+HVD: existential there + verb 'to have', past tense\n",
            "    there'd\n",
            "EX+HVZ: existential there + verb 'to have', present tense, 3rd person singular\n",
            "    there's\n",
            "EX+MD: existential there + modal auxillary\n",
            "    there'll there'd\n",
            "FW-*: foreign word: negator\n",
            "    pas non ne\n",
            "FW-AT: foreign word: article\n",
            "    la le el un die der ein keine eine das las les Il\n",
            "FW-AT+NN: foreign word: article + noun, singular, common\n",
            "    l'orchestre l'identite l'arcade l'ange l'assistance l'activite\n",
            "    L'Universite l'independance L'Union L'Unita l'osservatore\n",
            "FW-AT+NP: foreign word: article + noun, singular, proper\n",
            "    L'Astree L'Imperiale\n",
            "FW-BE: foreign word: verb 'to be', infinitive or imperative\n",
            "    sit\n",
            "FW-BER: foreign word: verb 'to be', present tense, 2nd person singular or all persons plural\n",
            "    sind sunt etes\n",
            "FW-BEZ: foreign word: verb 'to be', present tense, 3rd person singular\n",
            "    ist est\n",
            "FW-CC: foreign word: conjunction, coordinating\n",
            "    et ma mais und aber och nec y\n",
            "FW-CD: foreign word: numeral, cardinal\n",
            "    une cinq deux sieben unam zwei\n",
            "FW-CS: foreign word: conjunction, subordinating\n",
            "    bevor quam ma\n",
            "FW-DT: foreign word: determiner/pronoun, singular\n",
            "    hoc\n",
            "FW-DT+BEZ: foreign word: determiner + verb 'to be', present tense, 3rd person singular\n",
            "    c'est\n",
            "FW-DTS: foreign word: determiner/pronoun, plural\n",
            "    haec\n",
            "FW-HV: foreign word: verb 'to have', present tense, not 3rd person singular\n",
            "    habe\n",
            "FW-IN: foreign word: preposition\n",
            "    ad de en a par con dans ex von auf super post sine sur sub avec per\n",
            "    inter sans pour pendant in di\n",
            "FW-IN+AT: foreign word: preposition + article\n",
            "    della des du aux zur d'un del dell'\n",
            "FW-IN+NN: foreign word: preposition + noun, singular, common\n",
            "    d'etat d'hotel d'argent d'identite d'art\n",
            "FW-IN+NP: foreign word: preposition + noun, singular, proper\n",
            "    d'Yquem d'Eiffel\n",
            "FW-JJ: foreign word: adjective\n",
            "    avant Espagnol sinfonica Siciliana Philharmonique grand publique haute\n",
            "    noire bouffe Douce meme humaine bel serieuses royaux anticus presto\n",
            "    Sovietskaya Bayerische comique schwarzen ...\n",
            "FW-JJR: foreign word: adjective, comparative\n",
            "    fortiori\n",
            "FW-JJT: foreign word: adjective, superlative\n",
            "    optimo\n",
            "FW-NN: foreign word: noun, singular, common\n",
            "    ballet esprit ersatz mano chatte goutte sang Fledermaus oud def kolkhoz\n",
            "    roi troika canto boite blutwurst carne muzyka bonheur monde piece force\n",
            "    ...\n",
            "FW-NN$: foreign word: noun, singular, common, genitive\n",
            "    corporis intellectus arte's dei aeternitatis senioritatis curiae\n",
            "    patronne's chambre's\n",
            "FW-NNS: foreign word: noun, plural, common\n",
            "    al culpas vopos boites haflis kolkhozes augen tyrannis alpha-beta-\n",
            "    gammas metis banditos rata phis negociants crus Einsatzkommandos\n",
            "    kamikaze wohaws sabinas zorrillas palazzi engages coureurs corroborees\n",
            "    yori Ubermenschen ...\n",
            "FW-NP: foreign word: noun, singular, proper\n",
            "    Karshilama Dieu Rundfunk Afrique Espanol Afrika Spagna Gott Carthago\n",
            "    deus\n",
            "FW-NPS: foreign word: noun, plural, proper\n",
            "    Svenskarna Atlantes Dieux\n",
            "FW-NR: foreign word: noun, singular, adverbial\n",
            "    heute morgen aujourd'hui hoy\n",
            "FW-OD: foreign word: numeral, ordinal\n",
            "    18e 17e quintus\n",
            "FW-PN: foreign word: pronoun, nominal\n",
            "    hoc\n",
            "FW-PP$: foreign word: determiner, possessive\n",
            "    mea mon deras vos\n",
            "FW-PPL: foreign word: pronoun, singular, reflexive\n",
            "    se\n",
            "FW-PPL+VBZ: foreign word: pronoun, singular, reflexive + verb, present tense, 3rd person singular\n",
            "    s'excuse s'accuse\n",
            "FW-PPO: pronoun, personal, accusative\n",
            "    lui me moi mi\n",
            "FW-PPO+IN: foreign word: pronoun, personal, accusative + preposition\n",
            "    mecum tecum\n",
            "FW-PPS: foreign word: pronoun, personal, nominative, 3rd person singular\n",
            "    il\n",
            "FW-PPSS: foreign word: pronoun, personal, nominative, not 3rd person singular\n",
            "    ich vous sie je\n",
            "FW-PPSS+HV: foreign word: pronoun, personal, nominative, not 3rd person singular + verb 'to have', present tense, not 3rd person singular\n",
            "    j'ai\n",
            "FW-QL: foreign word: qualifier\n",
            "    minus\n",
            "FW-RB: foreign word: adverb\n",
            "    bas assai deja um wiederum cito velociter vielleicht simpliciter non zu\n",
            "    domi nuper sic forsan olim oui semper tout despues hors\n",
            "FW-RB+CC: foreign word: adverb + conjunction, coordinating\n",
            "    forisque\n",
            "FW-TO+VB: foreign word: infinitival to + verb, infinitive\n",
            "    d'entretenir\n",
            "FW-UH: foreign word: interjection\n",
            "    sayonara bien adieu arigato bonjour adios bueno tchalo ciao o\n",
            "FW-VB: foreign word: verb, present tense, not 3rd person singular, imperative or infinitive\n",
            "    nolo contendere vive fermate faciunt esse vade noli tangere dites duces\n",
            "    meminisse iuvabit gosaimasu voulez habla ksu'u'peli'afo lacheln miuchi\n",
            "    say allons strafe portant\n",
            "FW-VBD: foreign word: verb, past tense\n",
            "    stabat peccavi audivi\n",
            "FW-VBG: foreign word: verb, present participle or gerund\n",
            "    nolens volens appellant seq. obliterans servanda dicendi delenda\n",
            "FW-VBN: foreign word: verb, past participle\n",
            "    vue verstrichen rasa verboten engages\n",
            "FW-VBZ: foreign word: verb, present tense, 3rd person singular\n",
            "    gouverne sinkt sigue diapiace\n",
            "FW-WDT: foreign word: WH-determiner\n",
            "    quo qua quod que quok\n",
            "FW-WPO: foreign word: WH-pronoun, accusative\n",
            "    quibusdam\n",
            "FW-WPS: foreign word: WH-pronoun, nominative\n",
            "    qui\n",
            "HV: verb 'to have', uninflected present tense, infinitive or imperative\n",
            "    have hast\n",
            "HV*: verb 'to have', uninflected present tense or imperative, negated\n",
            "    haven't ain't\n",
            "HV+TO: verb 'to have', uninflected present tense + infinitival to\n",
            "    hafta\n",
            "HVD: verb 'to have', past tense\n",
            "    had\n",
            "HVD*: verb 'to have', past tense, negated\n",
            "    hadn't\n",
            "HVG: verb 'to have', present participle or gerund\n",
            "    having\n",
            "HVN: verb 'to have', past participle\n",
            "    had\n",
            "HVZ: verb 'to have', present tense, 3rd person singular\n",
            "    has hath\n",
            "HVZ*: verb 'to have', present tense, 3rd person singular, negated\n",
            "    hasn't ain't\n",
            "IN: preposition\n",
            "    of in for by considering to on among at through with under into\n",
            "    regarding than since despite according per before toward against as\n",
            "    after during including between without except upon out over ...\n",
            "IN+IN: preposition, hyphenated pair\n",
            "    f'ovuh\n",
            "IN+PPO: preposition + pronoun, personal, accusative\n",
            "    t'hi-im\n",
            "JJ: adjective\n",
            "    ecent over-all possible hard-fought favorable hard meager fit such\n",
            "    widespread outmoded inadequate ambiguous grand clerical effective\n",
            "    orderly federal foster general proportionate ...\n",
            "JJ$: adjective, genitive\n",
            "    Great's\n",
            "JJ+JJ: adjective, hyphenated pair\n",
            "    big-large long-far\n",
            "JJR: adjective, comparative\n",
            "    greater older further earlier later freer franker wider better deeper\n",
            "    firmer tougher faster higher bigger worse younger lighter nicer slower\n",
            "    happier frothier Greater newer Elder ...\n",
            "JJR+CS: adjective + conjunction, coordinating\n",
            "    lighter'n\n",
            "JJS: adjective, semantically superlative\n",
            "    top chief principal northernmost master key head main tops utmost\n",
            "    innermost foremost uppermost paramount topmost\n",
            "JJT: adjective, superlative\n",
            "    best largest coolest calmest latest greatest earliest simplest\n",
            "    strongest newest fiercest unhappiest worst youngest worthiest fastest\n",
            "    hottest fittest lowest finest smallest staunchest ...\n",
            "MD: modal auxillary\n",
            "    should may might will would must can could shall ought need wilt\n",
            "MD*: modal auxillary, negated\n",
            "    cannot couldn't wouldn't can't won't shouldn't shan't mustn't musn't\n",
            "MD+HV: modal auxillary + verb 'to have', uninflected form\n",
            "    shouldda musta coulda must've woulda could've\n",
            "MD+PPSS: modal auxillary + pronoun, personal, nominative, not 3rd person singular\n",
            "    willya\n",
            "MD+TO: modal auxillary + infinitival to\n",
            "    oughta\n",
            "NN: noun, singular, common\n",
            "    failure burden court fire appointment awarding compensation Mayor\n",
            "    interim committee fact effect airport management surveillance jail\n",
            "    doctor intern extern night weekend duty legislation Tax Office ...\n",
            "NN$: noun, singular, common, genitive\n",
            "    season's world's player's night's chapter's golf's football's\n",
            "    baseball's club's U.'s coach's bride's bridegroom's board's county's\n",
            "    firm's company's superintendent's mob's Navy's ...\n",
            "NN+BEZ: noun, singular, common + verb 'to be', present tense, 3rd person singular\n",
            "    water's camera's sky's kid's Pa's heat's throat's father's money's\n",
            "    undersecretary's granite's level's wife's fat's Knife's fire's name's\n",
            "    hell's leg's sun's roulette's cane's guy's kind's baseball's ...\n",
            "NN+HVD: noun, singular, common + verb 'to have', past tense\n",
            "    Pa'd\n",
            "NN+HVZ: noun, singular, common + verb 'to have', present tense, 3rd person singular\n",
            "    guy's Knife's boat's summer's rain's company's\n",
            "NN+IN: noun, singular, common + preposition\n",
            "    buncha\n",
            "NN+MD: noun, singular, common + modal auxillary\n",
            "    cowhand'd sun'll\n",
            "NN+NN: noun, singular, common, hyphenated pair\n",
            "    stomach-belly\n",
            "NNS: noun, plural, common\n",
            "    irregularities presentments thanks reports voters laws legislators\n",
            "    years areas adjustments chambers $100 bonds courts sales details raises\n",
            "    sessions members congressmen votes polls calls ...\n",
            "NNS$: noun, plural, common, genitive\n",
            "    taxpayers' children's members' States' women's cutters' motorists'\n",
            "    steelmakers' hours' Nations' lawyers' prisoners' architects' tourists'\n",
            "    Employers' secretaries' Rogues' ...\n",
            "NNS+MD: noun, plural, common + modal auxillary\n",
            "    duds'd oystchers'll\n",
            "NP: noun, singular, proper\n",
            "    Fulton Atlanta September-October Durwood Pye Ivan Allen Jr. Jan.\n",
            "    Alpharetta Grady William B. Hartsfield Pearl Williams Aug. Berry J. M.\n",
            "    Cheshire Griffin Opelika Ala. E. Pelham Snodgrass ...\n",
            "NP$: noun, singular, proper, genitive\n",
            "    Green's Landis' Smith's Carreon's Allison's Boston's Spahn's Willie's\n",
            "    Mickey's Milwaukee's Mays' Howsam's Mantle's Shaw's Wagner's Rickey's\n",
            "    Shea's Palmer's Arnold's Broglio's ...\n",
            "NP+BEZ: noun, singular, proper + verb 'to be', present tense, 3rd person singular\n",
            "    W.'s Ike's Mack's Jack's Kate's Katharine's Black's Arthur's Seaton's\n",
            "    Buckhorn's Breed's Penny's Rob's Kitty's Blackwell's Myra's Wally's\n",
            "    Lucille's Springfield's Arlene's\n",
            "NP+HVZ: noun, singular, proper + verb 'to have', present tense, 3rd person singular\n",
            "    Bill's Guardino's Celie's Skolman's Crosson's Tim's Wally's\n",
            "NP+MD: noun, singular, proper + modal auxillary\n",
            "    Gyp'll John'll\n",
            "NPS: noun, plural, proper\n",
            "    Chases Aderholds Chapelles Armisteads Lockies Carbones French Marskmen\n",
            "    Toppers Franciscans Romans Cadillacs Masons Blacks Catholics British\n",
            "    Dixiecrats Mississippians Congresses ...\n",
            "NPS$: noun, plural, proper, genitive\n",
            "    Republicans' Orioles' Birds' Yanks' Redbirds' Bucs' Yankees' Stevenses'\n",
            "    Geraghtys' Burkes' Wackers' Achaeans' Dresbachs' Russians' Democrats'\n",
            "    Gershwins' Adventists' Negroes' Catholics' ...\n",
            "NR: noun, singular, adverbial\n",
            "    Friday home Wednesday Tuesday Monday Sunday Thursday yesterday tomorrow\n",
            "    tonight West East Saturday west left east downtown north northeast\n",
            "    southeast northwest North South right ...\n",
            "NR$: noun, singular, adverbial, genitive\n",
            "    Saturday's Monday's yesterday's tonight's tomorrow's Sunday's\n",
            "    Wednesday's Friday's today's Tuesday's West's Today's South's\n",
            "NR+MD: noun, singular, adverbial + modal auxillary\n",
            "    today'll\n",
            "NRS: noun, plural, adverbial\n",
            "    Sundays Mondays Saturdays Wednesdays Souths Fridays\n",
            "OD: numeral, ordinal\n",
            "    first 13th third nineteenth 2d 61st second sixth eighth ninth twenty-\n",
            "    first eleventh 50th eighteenth- Thirty-ninth 72nd 1/20th twentieth\n",
            "    mid-19th thousandth 350th sixteenth 701st ...\n",
            "PN: pronoun, nominal\n",
            "    none something everything one anyone nothing nobody everybody everyone\n",
            "    anybody anything someone no-one nothin\n",
            "PN$: pronoun, nominal, genitive\n",
            "    one's someone's anybody's nobody's everybody's anyone's everyone's\n",
            "PN+BEZ: pronoun, nominal + verb 'to be', present tense, 3rd person singular\n",
            "    nothing's everything's somebody's nobody's someone's\n",
            "PN+HVD: pronoun, nominal + verb 'to have', past tense\n",
            "    nobody'd\n",
            "PN+HVZ: pronoun, nominal + verb 'to have', present tense, 3rd person singular\n",
            "    nobody's somebody's one's\n",
            "PN+MD: pronoun, nominal + modal auxillary\n",
            "    someone'll somebody'll anybody'd\n",
            "PP$: determiner, possessive\n",
            "    our its his their my your her out thy mine thine\n",
            "PP$$: pronoun, possessive\n",
            "    ours mine his hers theirs yours\n",
            "PPL: pronoun, singular, reflexive\n",
            "    itself himself myself yourself herself oneself ownself\n",
            "PPLS: pronoun, plural, reflexive\n",
            "    themselves ourselves yourselves\n",
            "PPO: pronoun, personal, accusative\n",
            "    them it him me us you 'em her thee we'uns\n",
            "PPS: pronoun, personal, nominative, 3rd person singular\n",
            "    it he she thee\n",
            "PPS+BEZ: pronoun, personal, nominative, 3rd person singular + verb 'to be', present tense, 3rd person singular\n",
            "    it's he's she's\n",
            "PPS+HVD: pronoun, personal, nominative, 3rd person singular + verb 'to have', past tense\n",
            "    she'd he'd it'd\n",
            "PPS+HVZ: pronoun, personal, nominative, 3rd person singular + verb 'to have', present tense, 3rd person singular\n",
            "    it's he's she's\n",
            "PPS+MD: pronoun, personal, nominative, 3rd person singular + modal auxillary\n",
            "    he'll she'll it'll he'd it'd she'd\n",
            "PPSS: pronoun, personal, nominative, not 3rd person singular\n",
            "    they we I you ye thou you'uns\n",
            "PPSS+BEM: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 1st person singular\n",
            "    I'm Ahm\n",
            "PPSS+BER: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 2nd person singular or all persons plural\n",
            "    we're you're they're\n",
            "PPSS+BEZ: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 3rd person singular\n",
            "    you's\n",
            "PPSS+BEZ*: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 3rd person singular, negated\n",
            "    'tain't\n",
            "PPSS+HV: pronoun, personal, nominative, not 3rd person singular + verb 'to have', uninflected present tense\n",
            "    I've we've they've you've\n",
            "PPSS+HVD: pronoun, personal, nominative, not 3rd person singular + verb 'to have', past tense\n",
            "    I'd you'd we'd they'd\n",
            "PPSS+MD: pronoun, personal, nominative, not 3rd person singular + modal auxillary\n",
            "    you'll we'll I'll we'd I'd they'll they'd you'd\n",
            "PPSS+VB: pronoun, personal, nominative, not 3rd person singular + verb 'to verb', uninflected present tense\n",
            "    y'know\n",
            "QL: qualifier, pre\n",
            "    well less very most so real as highly fundamentally even how much\n",
            "    remarkably somewhat more completely too thus ill deeply little overly\n",
            "    halfway almost impossibly far severly such ...\n",
            "QLP: qualifier, post\n",
            "    indeed enough still 'nuff\n",
            "RB: adverb\n",
            "    only often generally also nevertheless upon together back newly no\n",
            "    likely meanwhile near then heavily there apparently yet outright fully\n",
            "    aside consistently specifically formally ever just ...\n",
            "RB$: adverb, genitive\n",
            "    else's\n",
            "RB+BEZ: adverb + verb 'to be', present tense, 3rd person singular\n",
            "    here's there's\n",
            "RB+CS: adverb + conjunction, coordinating\n",
            "    well's soon's\n",
            "RBR: adverb, comparative\n",
            "    further earlier better later higher tougher more harder longer sooner\n",
            "    less faster easier louder farther oftener nearer cheaper slower tighter\n",
            "    lower worse heavier quicker ...\n",
            "RBR+CS: adverb, comparative + conjunction, coordinating\n",
            "    more'n\n",
            "RBT: adverb, superlative\n",
            "    most best highest uppermost nearest brightest hardest fastest deepest\n",
            "    farthest loudest ...\n",
            "RN: adverb, nominal\n",
            "    here afar then\n",
            "RP: adverb, particle\n",
            "    up out off down over on in about through across after\n",
            "RP+IN: adverb, particle + preposition\n",
            "    out'n outta\n",
            "TO: infinitival to\n",
            "    to t'\n",
            "TO+VB: infinitival to + verb, infinitive\n",
            "    t'jawn t'lah\n",
            "UH: interjection\n",
            "    Hurrah bang whee hmpf ah goodbye oops oh-the-pain-of-it ha crunch say\n",
            "    oh why see well hello lo alas tarantara rum-tum-tum gosh hell keerist\n",
            "    Jesus Keeeerist boy c'mon 'mon goddamn bah hoo-pig damn ...\n",
            "VB: verb, base: uninflected present, imperative or infinitive\n",
            "    investigate find act follow inure achieve reduce take remedy re-set\n",
            "    distribute realize disable feel receive continue place protect\n",
            "    eliminate elaborate work permit run enter force ...\n",
            "VB+AT: verb, base: uninflected present or infinitive + article\n",
            "    wanna\n",
            "VB+IN: verb, base: uninflected present, imperative or infinitive + preposition\n",
            "    lookit\n",
            "VB+JJ: verb, base: uninflected present, imperative or infinitive + adjective\n",
            "    die-dead\n",
            "VB+PPO: verb, uninflected present tense + pronoun, personal, accusative\n",
            "    let's lemme gimme\n",
            "VB+RP: verb, imperative + adverbial particle\n",
            "    g'ahn c'mon\n",
            "VB+TO: verb, base: uninflected present, imperative or infinitive + infinitival to\n",
            "    wanta wanna\n",
            "VB+VB: verb, base: uninflected present, imperative or infinitive; hypenated pair\n",
            "    say-speak\n",
            "VBD: verb, past tense\n",
            "    said produced took recommended commented urged found added praised\n",
            "    charged listed became announced brought attended wanted voted defeated\n",
            "    received got stood shot scheduled feared promised made ...\n",
            "VBG: verb, present participle or gerund\n",
            "    modernizing improving purchasing Purchasing lacking enabling pricing\n",
            "    keeping getting picking entering voting warning making strengthening\n",
            "    setting neighboring attending participating moving ...\n",
            "VBG+TO: verb, present participle + infinitival to\n",
            "    gonna\n",
            "VBN: verb, past participle\n",
            "    conducted charged won received studied revised operated accepted\n",
            "    combined experienced recommended effected granted seen protected\n",
            "    adopted retarded notarized selected composed gotten printed ...\n",
            "VBN+TO: verb, past participle + infinitival to\n",
            "    gotta\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    deserves believes receives takes goes expires says opposes starts\n",
            "    permits expects thinks faces votes teaches holds calls fears spends\n",
            "    collects backs eliminates sets flies gives seeks reads ...\n",
            "WDT: WH-determiner\n",
            "    which what whatever whichever whichever-the-hell\n",
            "WDT+BER: WH-determiner + verb 'to be', present tense, 2nd person singular or all persons plural\n",
            "    what're\n",
            "WDT+BER+PP: WH-determiner + verb 'to be', present, 2nd person singular or all persons plural + pronoun, personal, nominative, not 3rd person singular\n",
            "    whaddya\n",
            "WDT+BEZ: WH-determiner + verb 'to be', present tense, 3rd person singular\n",
            "    what's\n",
            "WDT+DO+PPS: WH-determiner + verb 'to do', uninflected present tense + pronoun, personal, nominative, not 3rd person singular\n",
            "    whaddya\n",
            "WDT+DOD: WH-determiner + verb 'to do', past tense\n",
            "    what'd\n",
            "WDT+HVZ: WH-determiner + verb 'to have', present tense, 3rd person singular\n",
            "    what's\n",
            "WP$: WH-pronoun, genitive\n",
            "    whose whosever\n",
            "WPO: WH-pronoun, accusative\n",
            "    whom that who\n",
            "WPS: WH-pronoun, nominative\n",
            "    that who whoever whosoever what whatsoever\n",
            "WPS+BEZ: WH-pronoun, nominative + verb 'to be', present, 3rd person singular\n",
            "    that's who's\n",
            "WPS+HVD: WH-pronoun, nominative + verb 'to have', past tense\n",
            "    who'd\n",
            "WPS+HVZ: WH-pronoun, nominative + verb 'to have', present tense, 3rd person singular\n",
            "    who's that's\n",
            "WPS+MD: WH-pronoun, nominative + modal auxillary\n",
            "    who'll that'd who'd that'll\n",
            "WQL: WH-qualifier\n",
            "    however how\n",
            "WRB: WH-adverb\n",
            "    however when where why whereby wherever how whenever whereon wherein\n",
            "    wherewith wheare wherefore whereof howsabout\n",
            "WRB+BER: WH-adverb + verb 'to be', present, 2nd person singular or all persons plural\n",
            "    where're\n",
            "WRB+BEZ: WH-adverb + verb 'to be', present, 3rd person singular\n",
            "    how's where's\n",
            "WRB+DO: WH-adverb + verb 'to do', present, not 3rd person singular\n",
            "    howda\n",
            "WRB+DOD: WH-adverb + verb 'to do', past tense\n",
            "    where'd how'd\n",
            "WRB+DOD*: WH-adverb + verb 'to do', past tense, negated\n",
            "    whyn't\n",
            "WRB+DOZ: WH-adverb + verb 'to do', present tense, 3rd person singular\n",
            "    how's\n",
            "WRB+IN: WH-adverb + preposition\n",
            "    why'n\n",
            "WRB+MD: WH-adverb + modal auxillary\n",
            "    where'd\n",
            "[('Good', 'JJ'), ('morning', 'NN'), ('everyone', 'NN'), ('.', '.')]\n",
            "[('Good', 'ADJ'), ('morning', 'NOUN'), ('everyone', 'NOUN'), ('.', '.')]\n",
            "[('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('study', 'VB'), ('NLTK', 'NNP'), ('.', '.')]\n",
            "[('Today', 'NOUN'), ('we', 'PRON'), ('will', 'VERB'), ('study', 'VERB'), ('NLTK', 'NOUN'), ('.', '.')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.help.brown_tagset()\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('tagsets')\n",
        "#text= state_union.raw(\"2005-GWBush.txt\")\n",
        "text=\"Good morning everyone. Today we will study NLTK.\"\n",
        "custom_sent_tokenizer1= sent_tokenize(text)\n",
        "#print(custom_sent_tokenizer1)\n",
        "custom_sent_tokenizer= PunktSentenceTokenizer(text)\n",
        "tokenized= custom_sent_tokenizer.tokenize(text)\n",
        "#print(tokenized)\n",
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized:\n",
        "      words= nltk.word_tokenize(i)\n",
        "      tagged= nltk.pos_tag(words)#('averaged perceptron tagger')\n",
        "      print(tagged)\n",
        "      tagged= nltk.pos_tag(words, tagset='universal')#('universal tagger')\n",
        "      print(tagged)\n",
        "  except Exception as e:\n",
        "   print(str(e))\n",
        "process_content()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w2Vw-OlYthGh",
        "outputId": "199ff602-6ed9-4bc7-8ea9-35c9d0e28a9b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cac39b5d15de>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maxent_ne_chunker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" Sundaram Pichai is a CEO of GOOGLE Company\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ],
      "source": [
        "from nltk import ne_chunk\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "#nltk.download('maxent_ne_chunker')\n",
        "#nltk.download('words')\n",
        "train_text=\" Sundaram Pichai is a CEO of GOOGLE Company\"\n",
        "sample_text=\" Sundaram Pichai is a CEO of GOOGLE Company\"\n",
        "custom_sent_tokenizer= PunktSentenceTokenizer(train_text)\n",
        "tokenized= custom_sent_tokenizer.tokenize(sample_text)\n",
        "print(tokenized)\n",
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized:\n",
        "      words= nltk.word_tokenize(i)\n",
        "      tagged= nltk.pos_tag(words)#('averaged perceptron tagger')\n",
        "      chunks=nltk.ne_chunk(tagged)\n",
        "      print(chunks)\n",
        "      chunks.pretty_print(unicodelines=True)\n",
        "  except Exception as e:\n",
        "   print(str(e))\n",
        "process_content()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwesY76m8X7R",
        "outputId": "1dde79fc-c19c-4f29-be92-f62946d65755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Zy0nBPz5f59",
        "outputId": "1acf0ec0-a171-4b3c-ccab-71eef078eb64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 205052 entries, 0 to 205051\n",
            "Data columns (total 6 columns):\n",
            " #   Column         Non-Null Count   Dtype \n",
            "---  ------         --------------   ----- \n",
            " 0   product_name   205052 non-null  object\n",
            " 1   product_price  205052 non-null  object\n",
            " 2   Rate           205052 non-null  object\n",
            " 3   Review         180388 non-null  object\n",
            " 4   Summary        205041 non-null  object\n",
            " 5   Sentiment      205052 non-null  object\n",
            "dtypes: object(6)\n",
            "memory usage: 9.4+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Dataset-SA.csv')\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oujb47mA5mYT",
        "outputId": "670bc9e8-8ab0-4101-ada2-0cf02c1c272c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['product_name', 'product_price', 'Rate', 'Review', 'Summary',\n",
            "       'Sentiment'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cello Pack of 18 Opalware Cello Dazzle Lush Fiesta Opalware Dinner Set, 18 Pieces Dinner SetÃÂ ÃÂ (White, Microwave Safe)    6005\n",
              "Lakm?? Eyeconic Kajal Twin Pack??????????(Deep Black, 0.7 g)                                                                   5000\n",
              "Mi 5A 80 cm (32 inch) HD Ready LED Smart Android TV with Dolby Audio (2022 Model)                                              2205\n",
              "cello Pack of 18 Opalware Cello Dazzle Lush Fiesta Opalware Dinner Set 18 Pieces Dinner SetWhite Microwave Safe                2095\n",
              "Home Sizzler 153 cm 502 ft Polyester Room Darkening Window Curtain Pack Of 2Floral Brown                                       2012\n",
              "                                                                                                                               ... \n",
              "LA VERNE Self Design Double Mink Blanket for  Heavy WinterWoollen Blend Pink                                                      5\n",
              "Stylish Brown Sports Men Watches Multifunctional Luxury Waterproof Modern Digital Watch   For Men                                 5\n",
              "VH000008C Analog Watch  - For Women                                                                                               4\n",
              "VH000008D Analog Watch  - For Women                                                                                               4\n",
              "LS2917 Mesh Strap All Black Avatar Day and Date Functioning Quartz Analog Watch   For Men                                         2\n",
              "Name: product_name, Length: 958, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "print(df.columns)\n",
        "df.product_name.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6NyWG3T5nI7"
      },
      "outputs": [],
      "source": [
        "df.loc[df['product_name'] == 'price', 'product_name'] = 'product_price'\n",
        "df.loc[df['product_name'] == 'rate', 'product_name'] = 'Rate'\n",
        "df.loc[df['product_name'] == 'review', 'product_name'] = 'Review'\n",
        "df.loc[df['product_name'] == 'summary', 'product_name'] = 'Summary'\n",
        "df.loc[df['product_name'] == 'sentiment', 'product_name'] = 'Sentiment'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "pF_GraYp9haC",
        "outputId": "c7676309-d8a9-4177-c227-8ccfa9409212"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"7e77292d-089b-46b4-aef4-83b3ab423c85\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7e77292d-089b-46b4-aef4-83b3ab423c85\")) {                    Plotly.newPlot(                        \"7e77292d-089b-46b4-aef4-83b3ab423c85\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"product_name=%{x}\\u003cbr\\u003eFeedback=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"cello Pack of 18 Opalware Cello Dazzle Lush Fiesta Opalware Dinner Set, 18 Pieces Dinner Set\\u00c3\\u0082\\u00c2\\u00a0\\u00c3\\u0082\\u00c2\\u00a0(White, Microwave Safe)\",\"Lakm?? Eyeconic Kajal Twin Pack??????????(Deep Black, 0.7 g)\",\"Mi 5A 80 cm (32 inch) HD Ready LED Smart Android TV with Dolby Audio (2022 Model)\",\"cello Pack of 18 Opalware Cello Dazzle Lush Fiesta Opalware Dinner Set 18 Pieces Dinner SetWhite Microwave Safe\",\"Home Sizzler 153 cm 502 ft Polyester Room Darkening Window Curtain Pack Of 2Floral Brown\",\"Mi 3i 10000 mAh Power Bank (Fast Charging, 18W)\\u00d0\\u0093\\u00d2\\u0093?\\u00d0\\u0093\\u00d3\\u00ae\\u00d0\\u0092\\u00c2\\u00a0\\u00d0\\u0093\\u00d2\\u0093?\\u00d0\\u0093\\u00d3\\u00ae\\u00d0\\u0092\\u00c2\\u00a0(Blue, Lithium Polymer)\",\"Singer FM 1409 Electric Sewing Machine\\u00d0\\u0093\\u00d2\\u0093?\\u00d0\\u0093\\u00d3\\u00ae\\u00d0\\u0092\\u00c2\\u00a0\\u00d0\\u0093\\u00d2\\u0093?\\u00d0\\u0093\\u00d3\\u00ae\\u00d0\\u0092\\u00c2\\u00a0( Built-in Stitches 9)\",\"Men Cargos\"],\"xaxis\":\"x\",\"y\":[6005,5000,2205,2095,2012,2006,2003,2000],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"product_name\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Feedback\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Customer Reviews\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7e77292d-089b-46b4-aef4-83b3ab423c85');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Assuming df['Product'].value_counts() is a Pandas Series\n",
        "product_counts = df['product_name'].value_counts()\n",
        "\n",
        "# Convert the Series to a DataFrame\n",
        "product_counts_df = product_counts.reset_index()\n",
        "product_counts_df.columns = ['product_name', 'Feedback']\n",
        "\n",
        "# Filter out products with less than 500 complaints\n",
        "product_counts_df = product_counts_df[product_counts_df['Feedback'] >= 2000]\n",
        "\n",
        "# Create a bar chart using Plotly\n",
        "fig = px.bar(product_counts_df, x='product_name', y='Feedback', title='Customer Reviews', labels={'Feedback': 'Feedback'})\n",
        "\n",
        "# Show the interactive plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrliCwYY-gST",
        "outputId": "d3b9b03e-e30f-45e3-85e8-32657f32a573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: 10\n",
            "product_name: Candes 60 L Room/Personal Air Cooler??????(White, Black, CRETA)\n",
            "product_price: 8999\n",
            "Rate: 5\n",
            "Review: great product\n",
            "Summary: beautiful product good material and perfectly working\n",
            "Sentiment: positive\n",
            "\n",
            "Index: 115\n",
            "product_name: MAHARAJA WHITELINE 65 L Desert Air Cooler??????(White, Grey, Rambo Grey / AC-303)\n",
            "product_price: 7999\n",
            "Rate: 4\n",
            "Review: good quality product\n",
            "Summary: really desert cooler super cool\n",
            "Sentiment: positive\n",
            "\n",
            "Index: 120\n",
            "product_name: MAHARAJA WHITELINE 65 L Desert Air Cooler??????(White, Grey, Rambo Grey / AC-303)\n",
            "product_price: 7999\n",
            "Rate: 3\n",
            "Review: just okay\n",
            "Summary: not so good\n",
            "Sentiment: negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def print_plots(indices):\n",
        "    for index in indices:\n",
        "        if index < len(df):\n",
        "            example = df[df.index == index][['product_name', 'product_price', 'Rate', 'Review', 'Summary', 'Sentiment']].values\n",
        "            if len(example) > 0:\n",
        "                print(\"Index:\", index)\n",
        "                print(\"product_name:\", example[0][0])\n",
        "                print('product_price:', example[0][1])\n",
        "                print('Rate:', example[0][2])\n",
        "                print('Review:', example[0][3])\n",
        "                print('Summary:', example[0][4])\n",
        "                print('Sentiment:', example[0][5])\n",
        "                print()  # Add a separator between entries\n",
        "            else:\n",
        "                print(\"No data found for index\", index)\n",
        "        else:\n",
        "            print(\"Index is out of range.\")\n",
        "\n",
        "# Call the function with a list of indices\n",
        "indices_to_print = [10, 115, 120]  # Example list of indices to print\n",
        "print_plots(indices_to_print)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W5vnrNV6YLy",
        "outputId": "c0e52c98-dd6f-4e00-dc9b-a3788822f50f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1320 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "# Create a function to handle conversion to string\n",
        "def convert_to_string(text):\n",
        "    return str(text) if isinstance(text, str) else ''\n",
        "\n",
        "# Apply the conversion function to the column\n",
        "df['Review'] = df['Review'].apply(convert_to_string)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(df['Review'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtQByUJV6dc2",
        "outputId": "6822da0b-7517-46dd-e842-d68928b19dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data tensor: (205052, 250)\n"
          ]
        }
      ],
      "source": [
        "X = tokenizer.texts_to_sequences(df['Rate'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd6dqrv_6g-v",
        "outputId": "eb9a8dc5-5409-4e97-9aaf-320b981c278c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (205052, 3)\n"
          ]
        }
      ],
      "source": [
        "Y = pd.get_dummies(df['Sentiment']).values\n",
        "print('Shape of label tensor:', Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhTU7qxO6j6o",
        "outputId": "5a5abaa2-8532-409f-a158-8cf850bc301a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(184546, 250) (184546, 3)\n",
            "(20506, 250) (20506, 3)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Training\n",
        "epochs = 3\n",
        "batch_size = 128\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
        "\n",
        "# Evaluation\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f\"Test loss: {score[0]}, Test accuracy: {score[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "gxa8qD2q88AB",
        "outputId": "40823b54-78a1-4060-dc64-12c0ebd87b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            " 197/1298 [===>..........................] - ETA: 22:12 - loss: 0.3791 - accuracy: 0.8828"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1febd69afaa2>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         ):\n\u001b[1;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    855\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfRJCL4UbsJE"
      },
      "outputs": [],
      "source": [
        "accr = model.evaluate(X_test,Y_test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAF9Oa_WbvyI"
      },
      "outputs": [],
      "source": [
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fLaw_Hebytv"
      },
      "outputs": [],
      "source": [
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['acc'], label='train')\n",
        "plt.plot(history.history['val_acc'], label='test')\n",
        "plt.legend()\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g-vZ9v6b1WG"
      },
      "outputs": [],
      "source": [
        "new_complaint = ['I am a victim of identity theft and someone stole my identity and personal information to open up a Visa credit card account with Bank of America. The following Bank of America Visa credit card account do not belong to me : XXXX.']\n",
        "seq = tokenizer.texts_to_sequences(new_complaint)\n",
        "padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "pred = model.predict(padded)\n",
        "labels = ['product_name', 'product_price', 'Rate', 'Review','Summary','Sentiment']\n",
        "print(pred, labels[np.argmax(pred)])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}